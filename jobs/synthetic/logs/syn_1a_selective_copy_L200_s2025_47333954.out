Starting job: syn_1a_selective_copy_L200_s2025
Date: Sat 03 Jan 2026 09:44:21 PM PST
Host: nid008288
GPU: NVIDIA A100-SXM4-80GB
Using GPU: NVIDIA A100-SXM4-80GB
================================================================================
SYNTHETIC BENCHMARK - SELECTIVE_COPY
================================================================================
Model ID: 1a (QuantumTransformer)
  Group 1: quantum feat → classical mix (transformer)
Task: selective_copy (regression)
Sequence Length: 200
Seed: 2025
Device: cuda
--------------------------------------------------------------------------------
Hyperparameters:
  n_qubits=6, n_layers=2
  d_model=128, d_state=16
  epochs=100, batch_size=32
  lr=0.001, weight_decay=0.0001
  early_stopping=20
================================================================================

Loading Data...
Dataset not found at data/synthetic_benchmarks/selective_copy/selective_copy_L200_M8_seed2025.pt. Generating...
Generating Selective Copy dataset...
  - num_samples: 5000
  - seq_len: 200
  - num_markers: 8
  - marker_strategy: uniform
  - marker_density: 4.0%

Dataset Statistics:
  - Input shape: torch.Size([5000, 200, 2])
  - Target shape: torch.Size([5000, 8])
  - Target value range: [0.0001, 1.0000]
  - Baseline MSE (predict 0.5): 0.0839

Dataset saved to data/synthetic_benchmarks/selective_copy/selective_copy_L200_M8_seed2025.pt
Loading Selective Copy dataset from data/synthetic_benchmarks/selective_copy/selective_copy_L200_M8_seed2025.pt...
Dataset loaded successfully.
  - Task: selective_copy
  - Sequence length: 200
  - Num channels: 2
  - Num markers: 8
  - Output length: 8
  - Total samples: 5000
  - Marker density: 4.0%
  - Baseline MSE: 0.0839
  - Input shape for model: torch.Size([5000, 2, 200])
  - Target shape: torch.Size([5000, 8])
  - Training set: 4000
  - Validation set: 500
  - Test set: 500
Data loaded!
  Input: (2 channels, 200 timesteps)
  Output dim: 8

Creating model...
Creating model: QuantumTransformer (ID: 1a)
  Group 1: quantum features → classical mixing (transformer)
Model parameters: 336,745
Verifying device placement...
  WARNING: pos_encoding is on cuda:0, moving to cuda
  WARNING: feature_proj.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch1.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch2.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch3.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.norm1.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.norm2.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.q_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.q_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.k_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.k_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.v_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.v_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.in_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.in_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.norm1.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.norm2.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.q_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.q_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.k_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.k_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.v_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.v_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.in_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.in_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.out_proj.bias is on cuda:0, moving to cuda
  WARNING: final_norm.weight is on cuda:0, moving to cuda
  WARNING: classifier.weight is on cuda:0, moving to cuda
  WARNING: classifier.bias is on cuda:0, moving to cuda
Model moved to cuda

================================================================================
Training Started (epochs 1 to 100)
================================================================================

Epoch   1/100 (35.3s) * | Train MSE: 0.102949 | Val MSE: 0.085753
Epoch   3/100 (34.6s) * | Train MSE: 0.088923 | Val MSE: 0.084221
Epoch   4/100 (34.8s) * | Train MSE: 0.087501 | Val MSE: 0.083452
Epoch   5/100 (34.6s) | Train MSE: 0.086545 | Val MSE: 0.083829
Epoch  10/100 (34.7s) * | Train MSE: 0.085706 | Val MSE: 0.083227
Epoch  11/100 (34.7s) * | Train MSE: 0.084860 | Val MSE: 0.080120
Epoch  12/100 (33.1s) * | Train MSE: 0.083750 | Val MSE: 0.075828
Epoch  13/100 (32.9s) * | Train MSE: 0.078263 | Val MSE: 0.069223
Epoch  15/100 (33.1s) | Train MSE: 0.072247 | Val MSE: 0.070186
Epoch  16/100 (32.9s) * | Train MSE: 0.071462 | Val MSE: 0.063375
Epoch  18/100 (33.0s) * | Train MSE: 0.069504 | Val MSE: 0.060896
Epoch  20/100 (33.1s) | Train MSE: 0.068021 | Val MSE: 0.066864
Epoch  22/100 (33.1s) * | Train MSE: 0.067095 | Val MSE: 0.057072
Epoch  24/100 (33.0s) * | Train MSE: 0.065312 | Val MSE: 0.056090
Epoch  25/100 (33.1s) | Train MSE: 0.064778 | Val MSE: 0.059365
Epoch  28/100 (33.0s) * | Train MSE: 0.061450 | Val MSE: 0.049788
Epoch  30/100 (33.1s) | Train MSE: 0.060965 | Val MSE: 0.055557
Epoch  32/100 (32.9s) * | Train MSE: 0.059738 | Val MSE: 0.048609
Epoch  34/100 (32.8s) * | Train MSE: 0.058653 | Val MSE: 0.047627
Epoch  35/100 (33.0s) | Train MSE: 0.058671 | Val MSE: 0.048884
Epoch  40/100 (32.9s) | Train MSE: 0.056630 | Val MSE: 0.048333
Epoch  41/100 (33.1s) * | Train MSE: 0.056681 | Val MSE: 0.046771
Epoch  43/100 (33.1s) * | Train MSE: 0.056557 | Val MSE: 0.045640
Epoch  44/100 (33.1s) * | Train MSE: 0.055657 | Val MSE: 0.043481
Epoch  45/100 (33.0s) | Train MSE: 0.054584 | Val MSE: 0.045302
Epoch  46/100 (33.1s) * | Train MSE: 0.054975 | Val MSE: 0.043177
Epoch  50/100 (32.9s) | Train MSE: 0.053247 | Val MSE: 0.045275
Epoch  54/100 (33.3s) * | Train MSE: 0.053349 | Val MSE: 0.042601
Epoch  55/100 (33.2s) * | Train MSE: 0.052837 | Val MSE: 0.039782
Epoch  60/100 (33.4s) | Train MSE: 0.050977 | Val MSE: 0.040752
Epoch  61/100 (33.2s) * | Train MSE: 0.051316 | Val MSE: 0.039654
Epoch  65/100 (33.3s) * | Train MSE: 0.050565 | Val MSE: 0.039422
Epoch  68/100 (33.3s) * | Train MSE: 0.049863 | Val MSE: 0.038535
Epoch  70/100 (33.3s) | Train MSE: 0.049962 | Val MSE: 0.039391
Epoch  71/100 (33.7s) * | Train MSE: 0.049239 | Val MSE: 0.038073
Epoch  75/100 (33.3s) * | Train MSE: 0.048685 | Val MSE: 0.037409
Epoch  79/100 (33.2s) * | Train MSE: 0.047943 | Val MSE: 0.037184
Epoch  80/100 (33.4s) | Train MSE: 0.048455 | Val MSE: 0.039366
Epoch  85/100 (33.3s) | Train MSE: 0.047837 | Val MSE: 0.039147
Epoch  90/100 (33.5s) | Train MSE: 0.046899 | Val MSE: 0.037661
Epoch  95/100 (33.1s) | Train MSE: 0.047056 | Val MSE: 0.038027

Early stopping at epoch 99 (no improvement for 20 epochs)

================================================================================
Training Complete!
================================================================================
Total Time: 3305.16s (55.09 min)
Best Val MSE: 0.037184
Test Results:
  MSE: 0.037889
  MAE: 0.150307
  R²: 0.5466
  Baseline MSE: 0.0830
  Improvement over baseline: 54.4%
Job completed: syn_1a_selective_copy_L200_s2025
End time: Sat 03 Jan 2026 10:40:07 PM PST
