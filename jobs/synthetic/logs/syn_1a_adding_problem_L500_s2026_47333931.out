Starting job: syn_1a_adding_problem_L500_s2026
Date: Sat 03 Jan 2026 03:23:06 PM PST
Host: nid008304
GPU: NVIDIA A100-SXM4-80GB
Using GPU: NVIDIA A100-SXM4-80GB
================================================================================
SYNTHETIC BENCHMARK - ADDING_PROBLEM
================================================================================
Model ID: 1a (QuantumTransformer)
  Group 1: quantum feat → classical mix (transformer)
Task: adding_problem (regression)
Sequence Length: 500
Seed: 2026
Device: cuda
--------------------------------------------------------------------------------
Hyperparameters:
  n_qubits=6, n_layers=2
  d_model=128, d_state=16
  epochs=100, batch_size=32
  lr=0.001, weight_decay=0.0001
  early_stopping=20
================================================================================

Loading Data...
Dataset not found at data/synthetic_benchmarks/adding_problem/adding_L500_seed2026.pt. Generating...
Generating Adding Problem dataset...
  - num_samples: 5000
  - seq_len: 500
  - marker_strategy: extremes

Dataset Statistics:
  - Target mean: 1.0144 (expected ~1.0)
  - Target std: 0.4003
  - Baseline MSE (predict 1.0): 0.1604
  - Random guess MSE: ~0.167

Dataset saved to data/synthetic_benchmarks/adding_problem/adding_L500_seed2026.pt
  - Sequences shape: torch.Size([5000, 500, 2])
  - Targets shape: torch.Size([5000, 1])
Loading Adding Problem dataset from data/synthetic_benchmarks/adding_problem/adding_L500_seed2026.pt...
Dataset loaded successfully.
  - Task: adding_problem
  - Sequence length: 500
  - Num channels: 2
  - Total samples: 5000
  - Marker strategy: extremes
  - Baseline MSE: 0.1604
  - Shape for model: torch.Size([5000, 2, 500])
  - Training set: 4000
  - Validation set: 500
  - Test set: 500
Data loaded!
  Input: (2 channels, 500 timesteps)
  Output dim: 1

Creating model...
Creating model: QuantumTransformer (ID: 1a)
  Group 1: quantum features → classical mixing (transformer)
Model parameters: 338,274
Verifying device placement...
  WARNING: pos_encoding is on cuda:0, moving to cuda
  WARNING: feature_proj.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch1.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch2.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch3.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.norm1.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.norm2.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.q_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.q_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.k_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.k_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.v_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.v_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.in_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.in_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.norm1.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.norm2.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.q_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.q_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.k_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.k_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.v_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.v_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.in_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.in_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.out_proj.bias is on cuda:0, moving to cuda
  WARNING: final_norm.weight is on cuda:0, moving to cuda
  WARNING: classifier.weight is on cuda:0, moving to cuda
  WARNING: classifier.bias is on cuda:0, moving to cuda
Model moved to cuda

================================================================================
Training Started (epochs 1 to 100)
================================================================================

Epoch   1/100 (35.8s) * | Train MSE: 0.202347 | Val MSE: 0.172134
Epoch   3/100 (33.8s) * | Train MSE: 0.168524 | Val MSE: 0.170185
Epoch   5/100 (33.9s) | Train MSE: 0.165302 | Val MSE: 0.171076
Epoch  10/100 (34.0s) | Train MSE: 0.163942 | Val MSE: 0.170828
Epoch  11/100 (33.8s) * | Train MSE: 0.164475 | Val MSE: 0.170133
Epoch  14/100 (33.9s) * | Train MSE: 0.162457 | Val MSE: 0.170115
Epoch  15/100 (33.9s) | Train MSE: 0.162898 | Val MSE: 0.183052
Epoch  20/100 (33.9s) | Train MSE: 0.162426 | Val MSE: 0.172696
Epoch  25/100 (33.9s) | Train MSE: 0.161519 | Val MSE: 0.172118
Epoch  26/100 (33.8s) * | Train MSE: 0.162010 | Val MSE: 0.170113
Epoch  30/100 (36.3s) | Train MSE: 0.161288 | Val MSE: 0.171695
Epoch  35/100 (33.8s) | Train MSE: 0.161412 | Val MSE: 0.170165
Epoch  40/100 (35.2s) * | Train MSE: 0.161583 | Val MSE: 0.170113
Epoch  44/100 (33.9s) * | Train MSE: 0.160740 | Val MSE: 0.140779
Epoch  45/100 (33.7s) * | Train MSE: 0.107110 | Val MSE: 0.100071
Epoch  47/100 (33.6s) * | Train MSE: 0.077447 | Val MSE: 0.077907
Epoch  48/100 (33.7s) * | Train MSE: 0.071947 | Val MSE: 0.046093
Epoch  50/100 (33.8s) | Train MSE: 0.073651 | Val MSE: 0.086904
Epoch  55/100 (33.4s) | Train MSE: 0.062088 | Val MSE: 0.082236
Epoch  60/100 (33.3s) | Train MSE: 0.048517 | Val MSE: 0.131030
Epoch  65/100 (33.2s) | Train MSE: 0.046055 | Val MSE: 0.137278

Early stopping at epoch 68 (no improvement for 20 epochs)

================================================================================
Training Complete!
================================================================================
Total Time: 2306.22s (38.44 min)
Best Val MSE: 0.046093
Test Results:
  MSE: 0.043854
  MAE: 0.180016
  R²: 0.7034
  Baseline MSE: 0.1670
  Improvement over baseline: 73.7%
Job completed: syn_1a_adding_problem_L500_s2026
End time: Sat 03 Jan 2026 04:01:50 PM PST
