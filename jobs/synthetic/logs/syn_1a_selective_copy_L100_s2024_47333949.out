Starting job: syn_1a_selective_copy_L100_s2024
Date: Sat 03 Jan 2026 08:36:59 PM PST
Host: nid008288
GPU: NVIDIA A100-SXM4-80GB
Using GPU: NVIDIA A100-SXM4-80GB
================================================================================
SYNTHETIC BENCHMARK - SELECTIVE_COPY
================================================================================
Model ID: 1a (QuantumTransformer)
  Group 1: quantum feat → classical mix (transformer)
Task: selective_copy (regression)
Sequence Length: 100
Seed: 2024
Device: cuda
--------------------------------------------------------------------------------
Hyperparameters:
  n_qubits=6, n_layers=2
  d_model=128, d_state=16
  epochs=100, batch_size=32
  lr=0.001, weight_decay=0.0001
  early_stopping=20
================================================================================

Loading Data...
Loading Selective Copy dataset from data/synthetic_benchmarks/selective_copy/selective_copy_L100_M8_seed2024.pt...
Dataset loaded successfully.
  - Task: selective_copy
  - Sequence length: 100
  - Num channels: 2
  - Num markers: 8
  - Output length: 8
  - Total samples: 5000
  - Marker density: 8.0%
  - Baseline MSE: 0.0828
  - Input shape for model: torch.Size([5000, 2, 100])
  - Target shape: torch.Size([5000, 8])
  - Training set: 4000
  - Validation set: 500
  - Test set: 500
Data loaded!
  Input: (2 channels, 100 timesteps)
  Output dim: 8

Creating model...
Creating model: QuantumTransformer (ID: 1a)
  Group 1: quantum features → classical mixing (transformer)
Model parameters: 335,977
Verifying device placement...
  WARNING: pos_encoding is on cuda:0, moving to cuda
  WARNING: feature_proj.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch1.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch2.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.branch3.base_params is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.norm1.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.norm2.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.q_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.q_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.k_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.k_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.v_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.v_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.attention.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.in_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.in_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.0.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.norm1.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.norm2.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.q_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.q_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.k_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.k_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.v_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.v_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.attention.out_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.in_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.in_proj.bias is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.out_proj.weight is on cuda:0, moving to cuda
  WARNING: transformer_layers.1.out_proj.bias is on cuda:0, moving to cuda
  WARNING: final_norm.weight is on cuda:0, moving to cuda
  WARNING: classifier.weight is on cuda:0, moving to cuda
  WARNING: classifier.bias is on cuda:0, moving to cuda
Model moved to cuda

================================================================================
Training Started (epochs 1 to 100)
================================================================================

Epoch   1/100 (35.6s) * | Train MSE: 0.093399 | Val MSE: 0.088157
Epoch   2/100 (34.5s) * | Train MSE: 0.087051 | Val MSE: 0.084774
Epoch   5/100 (33.0s) | Train MSE: 0.086370 | Val MSE: 0.087568
Epoch   6/100 (33.1s) * | Train MSE: 0.085974 | Val MSE: 0.084143
Epoch   9/100 (33.1s) * | Train MSE: 0.084370 | Val MSE: 0.083349
Epoch  10/100 (33.0s) | Train MSE: 0.084723 | Val MSE: 0.083468
Epoch  11/100 (32.7s) * | Train MSE: 0.083762 | Val MSE: 0.082734
Epoch  12/100 (32.9s) * | Train MSE: 0.078648 | Val MSE: 0.072350
Epoch  13/100 (32.5s) * | Train MSE: 0.070607 | Val MSE: 0.069378
Epoch  15/100 (32.6s) * | Train MSE: 0.067707 | Val MSE: 0.068455
Epoch  16/100 (32.5s) * | Train MSE: 0.066400 | Val MSE: 0.065530
Epoch  17/100 (32.6s) * | Train MSE: 0.065704 | Val MSE: 0.060726
Epoch  19/100 (32.5s) * | Train MSE: 0.064196 | Val MSE: 0.060595
Epoch  20/100 (32.5s) | Train MSE: 0.063539 | Val MSE: 0.061685
Epoch  21/100 (32.9s) * | Train MSE: 0.063421 | Val MSE: 0.059490
Epoch  22/100 (32.6s) * | Train MSE: 0.062264 | Val MSE: 0.057312
Epoch  25/100 (32.6s) | Train MSE: 0.060807 | Val MSE: 0.061756
Epoch  26/100 (32.3s) * | Train MSE: 0.061266 | Val MSE: 0.057082
Epoch  30/100 (32.5s) | Train MSE: 0.060379 | Val MSE: 0.064956
Epoch  35/100 (32.5s) | Train MSE: 0.059035 | Val MSE: 0.062093
Epoch  40/100 (32.9s) | Train MSE: 0.057714 | Val MSE: 0.063436
Epoch  45/100 (32.7s) | Train MSE: 0.056916 | Val MSE: 0.065410

Early stopping at epoch 46 (no improvement for 20 epochs)

================================================================================
Training Complete!
================================================================================
Total Time: 1508.25s (25.14 min)
Best Val MSE: 0.057082
Test Results:
  MSE: 0.057592
  MAE: 0.202023
  R²: 0.2865
  Baseline MSE: 0.0830
  Improvement over baseline: 30.6%
Job completed: syn_1a_selective_copy_L100_s2024
End time: Sat 03 Jan 2026 09:02:50 PM PST
