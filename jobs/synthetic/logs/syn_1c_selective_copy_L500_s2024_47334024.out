Starting job: syn_1c_selective_copy_L500_s2024
Date: Mon 05 Jan 2026 08:40:20 PM PST
Host: nid008288
GPU: NVIDIA A100-SXM4-80GB
Using GPU: NVIDIA A100-SXM4-80GB
================================================================================
SYNTHETIC BENCHMARK - SELECTIVE_COPY
================================================================================
Model ID: 1c (QuantumHydraSSM)
  Group 1: quantum feat → classical mix (hydra)
Task: selective_copy (regression)
Sequence Length: 500
Seed: 2024
Device: cuda
--------------------------------------------------------------------------------
Hyperparameters:
  n_qubits=6, n_layers=2
  d_model=128, d_state=16
  epochs=100, batch_size=32
  lr=0.001, weight_decay=0.0001
  early_stopping=20
================================================================================

Loading Data...
Loading Selective Copy dataset from data/synthetic_benchmarks/selective_copy/selective_copy_L500_M8_seed2024.pt...
Dataset loaded successfully.
  - Task: selective_copy
  - Sequence length: 500
  - Num channels: 2
  - Num markers: 8
  - Output length: 8
  - Total samples: 5000
  - Marker density: 1.6%
  - Baseline MSE: 0.0835
  - Input shape for model: torch.Size([5000, 2, 500])
  - Target shape: torch.Size([5000, 8])
  - Training set: 4000
  - Validation set: 500
  - Test set: 500
Data loaded!
  Input: (2 channels, 500 timesteps)
  Output dim: 8

Creating model...
Creating model: QuantumHydraSSM (ID: 1c)
  Group 1: quantum features → classical mixing (hydra)
Model parameters: 910,868
Verifying device placement...
  WARNING: alpha is on cuda:0, moving to cuda
  WARNING: beta is on cuda:0, moving to cuda
  WARNING: gamma is on cuda:0, moving to cuda
  WARNING: feature_proj.0.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.0.bias is on cuda:0, moving to cuda
  WARNING: feature_proj.1.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.in_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.alpha is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.beta is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.gamma is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.diagonal.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.in_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.alpha is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.beta is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.gamma is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.diagonal.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.norm.bias is on cuda:0, moving to cuda
  WARNING: output_norm.weight is on cuda:0, moving to cuda
  WARNING: output_norm.bias is on cuda:0, moving to cuda
  WARNING: output_layer.weight is on cuda:0, moving to cuda
  WARNING: output_layer.bias is on cuda:0, moving to cuda
Model moved to cuda

================================================================================
Training Started (epochs 1 to 100)
================================================================================

Epoch   1/100 (132.7s) * | Train MSE: 0.101392 | Val MSE: 0.083391
Epoch   3/100 (131.5s) * | Train MSE: 0.077406 | Val MSE: 0.075833
Epoch   4/100 (131.4s) * | Train MSE: 0.067413 | Val MSE: 0.058730
Epoch   5/100 (129.7s) * | Train MSE: 0.055906 | Val MSE: 0.050014
Epoch   6/100 (128.7s) * | Train MSE: 0.048805 | Val MSE: 0.048141
Epoch   7/100 (128.5s) * | Train MSE: 0.043904 | Val MSE: 0.040586
Epoch   8/100 (128.7s) * | Train MSE: 0.040644 | Val MSE: 0.039680
Epoch   9/100 (128.7s) * | Train MSE: 0.037950 | Val MSE: 0.035041
Epoch  10/100 (128.7s) | Train MSE: 0.036101 | Val MSE: 0.035818
Epoch  11/100 (128.5s) * | Train MSE: 0.033936 | Val MSE: 0.032608
Epoch  12/100 (128.5s) * | Train MSE: 0.031806 | Val MSE: 0.032358
Epoch  13/100 (128.5s) * | Train MSE: 0.031401 | Val MSE: 0.029786
Epoch  14/100 (128.5s) * | Train MSE: 0.029491 | Val MSE: 0.026918
Epoch  15/100 (128.5s) * | Train MSE: 0.029169 | Val MSE: 0.026884
Epoch  16/100 (128.5s) * | Train MSE: 0.026949 | Val MSE: 0.024727
Epoch  17/100 (130.9s) * | Train MSE: 0.025402 | Val MSE: 0.023428
Epoch  18/100 (131.6s) * | Train MSE: 0.025037 | Val MSE: 0.022848
Epoch  20/100 (131.6s) * | Train MSE: 0.022702 | Val MSE: 0.022785
Epoch  22/100 (127.5s) * | Train MSE: 0.022162 | Val MSE: 0.020911
Epoch  24/100 (127.6s) * | Train MSE: 0.019465 | Val MSE: 0.019269
Epoch  25/100 (127.5s) * | Train MSE: 0.018823 | Val MSE: 0.019239
Epoch  26/100 (127.5s) * | Train MSE: 0.018108 | Val MSE: 0.018628
Epoch  28/100 (127.5s) * | Train MSE: 0.017544 | Val MSE: 0.018308
Epoch  29/100 (127.5s) * | Train MSE: 0.016208 | Val MSE: 0.016942
Epoch  30/100 (127.5s) * | Train MSE: 0.015459 | Val MSE: 0.015349
Epoch  35/100 (127.2s) | Train MSE: 0.014332 | Val MSE: 0.015958
Epoch  40/100 (129.4s) * | Train MSE: 0.013078 | Val MSE: 0.014765
Epoch  41/100 (129.4s) * | Train MSE: 0.012175 | Val MSE: 0.014517
Epoch  42/100 (126.6s) * | Train MSE: 0.012134 | Val MSE: 0.014279
Epoch  45/100 (124.1s) | Train MSE: 0.011708 | Val MSE: 0.014362
Epoch  48/100 (123.4s) * | Train MSE: 0.011223 | Val MSE: 0.013593
Epoch  50/100 (123.5s) | Train MSE: 0.010830 | Val MSE: 0.015155
Epoch  53/100 (123.5s) * | Train MSE: 0.010074 | Val MSE: 0.013441
Epoch  55/100 (123.4s) | Train MSE: 0.009827 | Val MSE: 0.014716
Epoch  60/100 (123.5s) | Train MSE: 0.008586 | Val MSE: 0.014172
Epoch  65/100 (127.8s) | Train MSE: 0.007429 | Val MSE: 0.014002
Epoch  70/100 (125.0s) | Train MSE: 0.006410 | Val MSE: 0.014344

Early stopping at epoch 73 (no improvement for 20 epochs)

================================================================================
Training Complete!
================================================================================
Total Time: 9263.72s (154.40 min)
Best Val MSE: 0.013441
Test Results:
  MSE: 0.013357
  MAE: 0.072135
  R²: 0.8405
  Baseline MSE: 0.0830
  Improvement over baseline: 83.9%
Job completed: syn_1c_selective_copy_L500_s2024
End time: Mon 05 Jan 2026 11:15:07 PM PST
