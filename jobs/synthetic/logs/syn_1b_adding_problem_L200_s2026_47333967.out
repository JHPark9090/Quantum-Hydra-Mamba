Starting job: syn_1b_adding_problem_L200_s2026
Date: Sun 04 Jan 2026 05:50:34 AM PST
Host: nid008448
GPU: NVIDIA A100-SXM4-80GB
Using GPU: NVIDIA A100-SXM4-80GB
================================================================================
SYNTHETIC BENCHMARK - ADDING_PROBLEM
================================================================================
Model ID: 1b (QuantumMambaSSM)
  Group 1: quantum feat → classical mix (mamba)
Task: adding_problem (regression)
Sequence Length: 200
Seed: 2026
Device: cuda
--------------------------------------------------------------------------------
Hyperparameters:
  n_qubits=6, n_layers=2
  d_model=128, d_state=16
  epochs=100, batch_size=32
  lr=0.001, weight_decay=0.0001
  early_stopping=20
================================================================================

Loading Data...
Loading Adding Problem dataset from data/synthetic_benchmarks/adding_problem/adding_L200_seed2026.pt...
Dataset loaded successfully.
  - Task: adding_problem
  - Sequence length: 200
  - Num channels: 2
  - Total samples: 5000
  - Marker strategy: extremes
  - Baseline MSE: 0.1660
  - Shape for model: torch.Size([5000, 2, 200])
  - Training set: 4000
  - Validation set: 500
  - Test set: 500
Data loaded!
  Input: (2 channels, 200 timesteps)
  Output dim: 1

Creating model...
Creating model: QuantumMambaSSM (ID: 1b)
  Group 1: quantum features → classical mixing (mamba)
Model parameters: 421,730
Verifying device placement...
  WARNING: feature_proj.0.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.0.bias is on cuda:0, moving to cuda
  WARNING: feature_proj.1.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_processor.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.in_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.conv1d.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.conv1d.bias is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.ssm.A_log is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.ssm.D is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.out_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.norm.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.0.norm.bias is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.in_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.conv1d.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.conv1d.bias is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.ssm.A_log is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.ssm.D is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.out_proj.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.norm.weight is on cuda:0, moving to cuda
  WARNING: mamba_layers.1.norm.bias is on cuda:0, moving to cuda
  WARNING: output_norm.weight is on cuda:0, moving to cuda
  WARNING: output_norm.bias is on cuda:0, moving to cuda
  WARNING: output_layer.0.weight is on cuda:0, moving to cuda
  WARNING: output_layer.0.bias is on cuda:0, moving to cuda
  WARNING: output_layer.3.weight is on cuda:0, moving to cuda
  WARNING: output_layer.3.bias is on cuda:0, moving to cuda
Model moved to cuda

================================================================================
Training Started (epochs 1 to 100)
================================================================================

Epoch   1/100 (38.6s) * | Train MSE: 0.156892 | Val MSE: 0.032362
Epoch   2/100 (36.6s) * | Train MSE: 0.008113 | Val MSE: 0.000996
Epoch   5/100 (36.9s) * | Train MSE: 0.004165 | Val MSE: 0.000932
Epoch  10/100 (36.9s) | Train MSE: 0.003681 | Val MSE: 0.001175
Epoch  11/100 (37.0s) * | Train MSE: 0.003649 | Val MSE: 0.000614
Epoch  15/100 (36.9s) | Train MSE: 0.002820 | Val MSE: 0.002125
Epoch  17/100 (36.9s) * | Train MSE: 0.002819 | Val MSE: 0.000598
Epoch  20/100 (36.9s) * | Train MSE: 0.002615 | Val MSE: 0.000141
Epoch  23/100 (36.8s) * | Train MSE: 0.002263 | Val MSE: 0.000140
Epoch  25/100 (36.8s) | Train MSE: 0.002305 | Val MSE: 0.002620
Epoch  28/100 (37.1s) * | Train MSE: 0.002092 | Val MSE: 0.000131
Epoch  30/100 (37.0s) | Train MSE: 0.002010 | Val MSE: 0.000901
Epoch  35/100 (37.3s) | Train MSE: 0.001854 | Val MSE: 0.000396
Epoch  39/100 (37.1s) * | Train MSE: 0.001716 | Val MSE: 0.000105
Epoch  40/100 (37.1s) | Train MSE: 0.001838 | Val MSE: 0.000893
Epoch  45/100 (37.0s) | Train MSE: 0.001508 | Val MSE: 0.000139
Epoch  50/100 (37.2s) | Train MSE: 0.001413 | Val MSE: 0.000160
Epoch  53/100 (37.0s) * | Train MSE: 0.001500 | Val MSE: 0.000064
Epoch  55/100 (36.6s) * | Train MSE: 0.001370 | Val MSE: 0.000054
Epoch  60/100 (36.6s) | Train MSE: 0.001286 | Val MSE: 0.000243
Epoch  65/100 (36.6s) | Train MSE: 0.001207 | Val MSE: 0.000081
Epoch  69/100 (36.6s) * | Train MSE: 0.001229 | Val MSE: 0.000029
Epoch  70/100 (36.5s) | Train MSE: 0.001160 | Val MSE: 0.000157
Epoch  75/100 (36.6s) | Train MSE: 0.001026 | Val MSE: 0.000069
Epoch  80/100 (36.7s) | Train MSE: 0.001074 | Val MSE: 0.000265
Epoch  81/100 (36.7s) * | Train MSE: 0.001071 | Val MSE: 0.000014
Epoch  85/100 (36.8s) | Train MSE: 0.001040 | Val MSE: 0.000235
Epoch  90/100 (36.6s) | Train MSE: 0.000997 | Val MSE: 0.000100
Epoch  95/100 (36.6s) | Train MSE: 0.001006 | Val MSE: 0.000084
Epoch 100/100 (36.6s) | Train MSE: 0.000971 | Val MSE: 0.000098

================================================================================
Training Complete!
================================================================================
Total Time: 3683.02s (61.38 min)
Best Val MSE: 0.000014
Test Results:
  MSE: 0.000014
  MAE: 0.002913
  R²: 0.9999
  Baseline MSE: 0.1670
  Improvement over baseline: 100.0%
Job completed: syn_1b_adding_problem_L200_s2026
End time: Sun 04 Jan 2026 06:52:08 AM PST
