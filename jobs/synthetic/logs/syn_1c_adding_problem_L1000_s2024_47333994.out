Starting job: syn_1c_adding_problem_L1000_s2024
Date: Sun 04 Jan 2026 09:01:21 PM PST
Host: nid008229
GPU: NVIDIA A100-SXM4-80GB
Using GPU: NVIDIA A100-SXM4-80GB
================================================================================
SYNTHETIC BENCHMARK - ADDING_PROBLEM
================================================================================
Model ID: 1c (QuantumHydraSSM)
  Group 1: quantum feat → classical mix (hydra)
Task: adding_problem (regression)
Sequence Length: 1000
Seed: 2024
Device: cuda
--------------------------------------------------------------------------------
Hyperparameters:
  n_qubits=6, n_layers=2
  d_model=128, d_state=16
  epochs=100, batch_size=32
  lr=0.001, weight_decay=0.0001
  early_stopping=20
================================================================================

Loading Data...
Loading Adding Problem dataset from data/synthetic_benchmarks/adding_problem/adding_L1000_seed2024.pt...
Dataset loaded successfully.
  - Task: adding_problem
  - Sequence length: 1000
  - Num channels: 2
  - Total samples: 5000
  - Marker strategy: extremes
  - Baseline MSE: 0.1696
  - Shape for model: torch.Size([5000, 2, 1000])
  - Training set: 4000
  - Validation set: 500
  - Test set: 500
Data loaded!
  Input: (2 channels, 1000 timesteps)
  Output dim: 1

Creating model...
Creating model: QuantumHydraSSM (ID: 1c)
  Group 1: quantum features → classical mixing (hydra)
Model parameters: 909,965
Verifying device placement...
  WARNING: alpha is on cuda:0, moving to cuda
  WARNING: beta is on cuda:0, moving to cuda
  WARNING: gamma is on cuda:0, moving to cuda
  WARNING: feature_proj.0.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.0.bias is on cuda:0, moving to cuda
  WARNING: feature_proj.1.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.in_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.alpha is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.beta is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.gamma is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.diagonal.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.in_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.alpha is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.beta is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.gamma is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.diagonal.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.norm.bias is on cuda:0, moving to cuda
  WARNING: output_norm.weight is on cuda:0, moving to cuda
  WARNING: output_norm.bias is on cuda:0, moving to cuda
  WARNING: output_layer.weight is on cuda:0, moving to cuda
  WARNING: output_layer.bias is on cuda:0, moving to cuda
Model moved to cuda

================================================================================
Training Started (epochs 1 to 100)
================================================================================

Epoch   1/100 (144.4s) * | Train MSE: 0.216317 | Val MSE: 0.169681
Epoch   2/100 (139.8s) * | Train MSE: 0.169793 | Val MSE: 0.086154
Epoch   3/100 (141.4s) * | Train MSE: 0.034103 | Val MSE: 0.003541
Epoch   4/100 (141.3s) * | Train MSE: 0.011095 | Val MSE: 0.001408
Epoch   5/100 (139.8s) | Train MSE: 0.002132 | Val MSE: 0.001441
Epoch   7/100 (141.7s) * | Train MSE: 0.001063 | Val MSE: 0.001013
Epoch  10/100 (140.3s) | Train MSE: 0.000652 | Val MSE: 0.001761
Epoch  15/100 (140.5s) * | Train MSE: 0.000435 | Val MSE: 0.000739
Epoch  20/100 (140.4s) | Train MSE: 0.000394 | Val MSE: 0.001954
Epoch  22/100 (141.2s) * | Train MSE: 0.000386 | Val MSE: 0.000706
Epoch  25/100 (139.2s) | Train MSE: 0.000327 | Val MSE: 0.000883
Epoch  30/100 (135.3s) | Train MSE: 0.000274 | Val MSE: 0.001546
Epoch  31/100 (135.8s) * | Train MSE: 0.000226 | Val MSE: 0.000440
Epoch  35/100 (135.1s) | Train MSE: 0.000192 | Val MSE: 0.000592
Epoch  38/100 (132.7s) * | Train MSE: 0.000337 | Val MSE: 0.000158
Epoch  40/100 (132.0s) | Train MSE: 0.000178 | Val MSE: 0.001652
Epoch  45/100 (135.9s) | Train MSE: 0.000158 | Val MSE: 0.001518
Epoch  47/100 (135.5s) * | Train MSE: 0.000143 | Val MSE: 0.000142
Epoch  50/100 (135.1s) | Train MSE: 0.000136 | Val MSE: 0.000765
Epoch  55/100 (135.1s) | Train MSE: 0.000118 | Val MSE: 0.000418
Epoch  60/100 (142.1s) | Train MSE: 0.000119 | Val MSE: 0.000731
Epoch  65/100 (140.4s) | Train MSE: 0.000088 | Val MSE: 0.000423

Early stopping at epoch 67 (no improvement for 20 epochs)

================================================================================
Training Complete!
================================================================================
Total Time: 9241.59s (154.03 min)
Best Val MSE: 0.000142
Test Results:
  MSE: 0.000132
  MAE: 0.009731
  R²: 0.9992
  Baseline MSE: 0.1670
  Improvement over baseline: 99.9%
Job completed: syn_1c_adding_problem_L1000_s2024
End time: Sun 04 Jan 2026 11:35:44 PM PST
