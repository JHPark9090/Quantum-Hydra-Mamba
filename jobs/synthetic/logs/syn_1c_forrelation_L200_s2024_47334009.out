Starting job: syn_1c_forrelation_L200_s2024
Date: Mon 05 Jan 2026 02:09:31 PM PST
Host: nid008297
GPU: NVIDIA A100-SXM4-80GB
Using GPU: NVIDIA A100-SXM4-80GB
================================================================================
SYNTHETIC BENCHMARK - FORRELATION
================================================================================
Model ID: 1c (QuantumHydraSSM)
  Group 1: quantum feat → classical mix (hydra)
Task: forrelation (classification)
Sequence Length: 200
Seed: 2024
Device: cuda
--------------------------------------------------------------------------------
Hyperparameters:
  n_qubits=6, n_layers=2
  d_model=128, d_state=16
  epochs=100, batch_size=32
  lr=0.001, weight_decay=0.0001
  early_stopping=20
================================================================================

Loading Data...
Loading dataset from data/synthetic_benchmarks/forrelation/forrelation_L200_seed2024.pt...
Dataset loaded successfully.
  - n_bits: 6
  - seq_len: 200
  - num_channels: 14
  - Total sequences: 5000
  - Shape for model: torch.Size([5000, 14, 200])
  - Training set size: 4000
  - Test set size: 1000
Data loaded!
  Input: (14 channels, 200 timesteps)
  Output dim: 2

Creating model...
Creating model: QuantumHydraSSM (ID: 1c)
  Group 1: quantum features → classical mixing (hydra)
Model parameters: 911,630
Verifying device placement...
  WARNING: alpha is on cuda:0, moving to cuda
  WARNING: beta is on cuda:0, moving to cuda
  WARNING: gamma is on cuda:0, moving to cuda
  WARNING: feature_proj.0.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.0.bias is on cuda:0, moving to cuda
  WARNING: feature_proj.1.weight is on cuda:0, moving to cuda
  WARNING: feature_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_forward.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_backward.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.2.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.chunk_attention.2.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.alpha_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.alpha_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.beta_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.beta_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.gamma_real is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.gamma_imag is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj1.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj1.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj2.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj2.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj3.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.proj3.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.input_proj.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.quantum_branches.input_proj.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.0.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.0.bias is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.1.weight is on cuda:0, moving to cuda
  WARNING: quantum_diagonal.output_proj.1.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.in_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.alpha is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.beta is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.gamma is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.forward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.backward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.diagonal.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.bidirectional_ssm.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.0.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.in_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.alpha is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.beta is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.gamma is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.forward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.A_log is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.D is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.x_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.dt_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.dt_proj.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.backward_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.diagonal.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.bidirectional_ssm.norm.bias is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.out_proj.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.norm.weight is on cuda:0, moving to cuda
  WARNING: hydra_layers.1.norm.bias is on cuda:0, moving to cuda
  WARNING: output_norm.weight is on cuda:0, moving to cuda
  WARNING: output_norm.bias is on cuda:0, moving to cuda
  WARNING: output_layer.weight is on cuda:0, moving to cuda
  WARNING: output_layer.bias is on cuda:0, moving to cuda
Model moved to cuda

================================================================================
Training Started (epochs 1 to 100)
================================================================================

Epoch   1/100 (122.2s) * | Train: 0.0179/0.9942 | Val: 0.0000/1.0000
Epoch   5/100 (118.6s) | Train: 0.0000/1.0000 | Val: 0.0000/1.0000
Epoch  10/100 (120.8s) | Train: 0.0000/1.0000 | Val: 0.0000/1.0000
Epoch  15/100 (119.4s) | Train: 0.0000/1.0000 | Val: 0.0000/1.0000
Epoch  20/100 (119.8s) | Train: 0.0000/1.0000 | Val: 0.0000/1.0000

Early stopping at epoch 21 (no improvement for 20 epochs)

================================================================================
Training Complete!
================================================================================
Total Time: 2504.38s (41.74 min)
Best Val Acc: 1.0000
Test Results:
  Accuracy: 1.0000
  AUC: 1.0000
  F1: 1.0000
  Baseline: 0.5000

Results saved to: results/synthetic_benchmarks/synthetic_1c_forrelation_L200_seed2024_results.json
Model saved to: results/synthetic_benchmarks/synthetic_1c_forrelation_L200_seed2024_model.pt
Cleaned up training checkpoint: results/synthetic_benchmarks/checkpoints/checkpoint_1c_forrelation_L200_seed2024.pt

================================================================================
SYNTHETIC BENCHMARK RUN COMPLETE
================================================================================
Model: 1c (QuantumHydraSSM)
Task: forrelation
Seq Length: 200
Test Accuracy: 1.0000
Test AUC: 1.0000
================================================================================

Job completed: syn_1c_forrelation_L200_s2024
End time: Mon 05 Jan 2026 02:51:45 PM PST
