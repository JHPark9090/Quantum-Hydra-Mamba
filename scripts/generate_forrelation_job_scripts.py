#!/usr/bin/env python3
"""
Generate SLURM job scripts for Forrelation experiments.

This script creates job submission scripts for all models on all datasets,
following the Quantum Advantage Test Plan.
"""

from pathlib import Path

# Configuration
MODELS = [
    'quantum_hydra',
    'quantum_hydra_hybrid',
    'quantum_mamba',
    'quantum_mamba_hybrid',
    'classical_hydra',
    'classical_mamba'
]

# Datasets for Phase 3 (Silver Standard - Sample Efficiency)
PHASE3_DATASETS = [
    'forrelation_L20.pt',
    'forrelation_L40.pt',
    'forrelation_L80.pt',
    'forrelation_L160.pt'
]

# Datasets for Phase 4 (Gold Standard - Scaling)
PHASE4_DATASETS = [
    'forrelation_n8_L40.pt',
    'forrelation_n8_L80.pt',
    'forrelation_n8_L160.pt',
    'forrelation_n8_L320.pt'
]

SEEDS = [2024, 2025, 2026]  # 3 seeds for statistical significance

# Hyperparameters (optimized from Phase 1 tuning - using reasonable defaults)
HYPERPARAMS = {
    'quantum_hydra': {'n_qubits': 6, 'qlcu_layers': 2, 'd_model': 128, 'd_state': 16, 'lr': 1e-3},
    'quantum_hydra_hybrid': {'n_qubits': 6, 'qlcu_layers': 2, 'd_model': 128, 'd_state': 16, 'lr': 1e-3},
    'quantum_mamba': {'n_qubits': 6, 'qlcu_layers': 2, 'd_model': 128, 'd_state': 16, 'lr': 1e-3},
    'quantum_mamba_hybrid': {'n_qubits': 6, 'qlcu_layers': 2, 'd_model': 128, 'd_state': 16, 'lr': 1e-3},
    'classical_hydra': {'n_qubits': 6, 'qlcu_layers': 2, 'd_model': 128, 'd_state': 16, 'lr': 1e-3},
    'classical_mamba': {'n_qubits': 6, 'qlcu_layers': 2, 'd_model': 128, 'd_state': 16, 'lr': 1e-3},
}

SLURM_TEMPLATE = """#!/bin/bash
#SBATCH --job-name={job_name}
#SBATCH --account=m4138_g
#SBATCH --constraint=gpu&hbm80g
#SBATCH --qos=shared
#SBATCH -t 48:00:00
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=32
#SBATCH --output=./experiments/forrelation_results/logs/{log_file}
#SBATCH --error=./experiments/forrelation_results/logs/{log_file}

# Job script for Forrelation: {model_name} on {dataset_name} (seed={seed})
# Auto-generated by generate_forrelation_job_scripts.py

# Activate conda environment
source activate ./conda-envs/qml_env

# Run training
python experiments/run_single_model_forrelation.py \\
    --model-name {model_name} \\
    --dataset-path forrelation_data/{dataset_file} \\
    --n-qubits {n_qubits} \\
    --qlcu-layers {qlcu_layers} \\
    --d-model {d_model} \\
    --d-state {d_state} \\
    --n-epochs 100 \\
    --batch-size 32 \\
    --lr {lr} \\
    --early-stopping-patience 15 \\
    --seed {seed} \\
    --output-dir ./experiments/forrelation_results \\
    --device cuda

echo "Job completed: {model_name} on {dataset_name} seed={seed}"
"""


def generate_job_script(model_name, dataset_file, seed, output_dir):
    """Generate a single SLURM job script."""

    # Extract dataset name from filename
    dataset_name = dataset_file.replace('.pt', '').replace('forrelation_', '')

    # Get hyperparameters for this model
    params = HYPERPARAMS[model_name]

    # Create job name and log file name
    job_name = f"forr_{model_name[:8]}_{dataset_name}_s{seed}"
    log_file = f"forrelation_{model_name}_{dataset_name}_seed{seed}.log"

    # Fill in template
    script_content = SLURM_TEMPLATE.format(
        job_name=job_name,
        log_file=log_file,
        model_name=model_name,
        dataset_name=dataset_name,
        dataset_file=dataset_file,
        seed=seed,
        **params
    )

    # Create output filename
    script_filename = output_dir / f"forrelation_{model_name}_{dataset_name}_seed{seed}.sh"

    # Write script
    with open(script_filename, 'w') as f:
        f.write(script_content)

    # Make executable
    script_filename.chmod(0o755)

    return script_filename


def main():
    print("="*80)
    print("GENERATING FORRELATION JOB SCRIPTS")
    print("="*80)
    print()

    # Create output directory
    output_dir = Path("experiments/forrelation_job_scripts")
    output_dir.mkdir(parents=True, exist_ok=True)

    # Create logs directory
    logs_dir = Path("experiments/forrelation_results/logs")
    logs_dir.mkdir(parents=True, exist_ok=True)

    # Combine all datasets
    all_datasets = PHASE3_DATASETS + PHASE4_DATASETS

    total_jobs = 0

    print(f"Generating job scripts for:")
    print(f"  Models: {len(MODELS)}")
    print(f"  Datasets: {len(all_datasets)}")
    print(f"  Seeds: {len(SEEDS)}")
    print(f"  Total jobs: {len(MODELS) * len(all_datasets) * len(SEEDS)}")
    print()

    # Generate scripts for Phase 3 (Silver Standard)
    print("Phase 3 (Silver Standard - Sample Efficiency):")
    for dataset in PHASE3_DATASETS:
        print(f"  Dataset: {dataset}")
        for model in MODELS:
            for seed in SEEDS:
                script_file = generate_job_script(model, dataset, seed, output_dir)
                total_jobs += 1
    print()

    # Generate scripts for Phase 4 (Gold Standard)
    print("Phase 4 (Gold Standard - Scaling):")
    for dataset in PHASE4_DATASETS:
        print(f"  Dataset: {dataset}")
        for model in MODELS:
            for seed in SEEDS:
                script_file = generate_job_script(model, dataset, seed, output_dir)
                total_jobs += 1
    print()

    print("="*80)
    print(f"JOB SCRIPTS GENERATED SUCCESSFULLY!")
    print("="*80)
    print(f"Total scripts: {total_jobs}")
    print(f"Output directory: {output_dir}")
    print()
    print("Generated script files:")
    scripts = sorted(output_dir.glob("*.sh"))
    for i, script in enumerate(scripts[:10]):
        print(f"  {script.name}")
    if len(scripts) > 10:
        print(f"  ... and {len(scripts) - 10} more")
    print()
    print("Next step: Submit jobs with:")
    print("  bash experiments/submit_forrelation_experiments.sh")
    print("="*80)


if __name__ == "__main__":
    main()
