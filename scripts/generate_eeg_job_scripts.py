#!/usr/bin/env python3
"""
Generate 30 individual job scripts for Tier 1 Experiment 1.1: EEG Classification
6 models Ã— 5 seeds = 30 jobs
"""

from pathlib import Path

# Configuration
MODELS = [
    'quantum_hydra',
    'quantum_hydra_hybrid',
    'quantum_mamba',
    'quantum_mamba_hybrid',
    'classical_hydra',
    'classical_mamba'
]

SEEDS = [2024, 2025, 2026, 2027, 2028]

# Hyperparameters from EXPERIMENTAL_PLAN_README.md (lines 468-478)
N_QUBITS = 6
QLCU_LAYERS = 2
D_MODEL = 128
D_STATE = 16
N_EPOCHS = 50
BATCH_SIZE = 32  # Updated for faster training
LEARNING_RATE = 1e-3
EARLY_STOPPING = 10  # Early stopping patience
SAMPLE_SIZE = 50  # Updated to 50 subjects for better statistical power
SAMPLING_FREQ = 80  # Updated to 80 Hz (balanced speed/quality)

OUTPUT_DIR = "./experiments/eeg_results"
CONDA_ENV = "./conda-envs/qml_env"

def create_job_script(model_name, seed, job_id):
    """Create a single job script for (model, seed) pair."""

    script_content = f"""#!/bin/bash
#SBATCH --job-name=eeg_{model_name}_s{seed}
#SBATCH --account=m4138_g
#SBATCH --constraint=gpu&hbm80g
#SBATCH --qos=shared
#SBATCH -t 48:00:00
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=32
#SBATCH --output={OUTPUT_DIR}/logs/eeg_{model_name}_seed{seed}.log
#SBATCH --error={OUTPUT_DIR}/logs/eeg_{model_name}_seed{seed}.log

# Job script for EEG Classification: {model_name} (seed={seed})
# Auto-generated by generate_eeg_job_scripts.py

# Activate conda environment
source activate {CONDA_ENV}

# Run training
python experiments/run_single_model_eeg.py \\
    --model-name {model_name} \\
    --n-qubits {N_QUBITS} \\
    --qlcu-layers {QLCU_LAYERS} \\
    --d-model {D_MODEL} \\
    --d-state {D_STATE} \\
    --n-epochs {N_EPOCHS} \\
    --batch-size {BATCH_SIZE} \\
    --lr {LEARNING_RATE} \\
    --early-stopping-patience {EARLY_STOPPING} \\
    --sample-size {SAMPLE_SIZE} \\
    --sampling-freq {SAMPLING_FREQ} \\
    --seed {seed} \\
    --output-dir {OUTPUT_DIR} \\
    --device cuda

echo "Job completed: {model_name} seed={seed}"
"""

    return script_content


def main():
    """Generate all 30 job scripts."""

    # Create job scripts directory
    jobs_dir = Path("./experiments/eeg_job_scripts")
    jobs_dir.mkdir(parents=True, exist_ok=True)

    print("="*80)
    print("Generating EEG Classification Job Scripts")
    print("="*80)
    print(f"Models: {len(MODELS)}")
    print(f"Seeds: {len(SEEDS)}")
    print(f"Total jobs: {len(MODELS) * len(SEEDS)}")
    print(f"Output directory: {jobs_dir}")
    print("="*80)

    job_files = []
    job_id = 1

    for model_name in MODELS:
        for seed in SEEDS:
            # Create job script
            script_content = create_job_script(model_name, seed, job_id)

            # Save to file
            job_filename = f"eeg_{model_name}_seed{seed}.sh"
            job_path = jobs_dir / job_filename

            with open(job_path, 'w') as f:
                f.write(script_content)

            # Make executable
            job_path.chmod(0o755)

            job_files.append(job_filename)
            print(f"[{job_id:2d}/30] Created: {job_filename}")
            job_id += 1

    print("\n" + "="*80)
    print("Job Scripts Generation Complete!")
    print("="*80)

    # Create master submission script
    master_script = """#!/bin/bash
# Master submission script for all 30 EEG classification jobs
# Run all jobs in parallel (if you have GPU resources)
# Or submit in batches

echo "Submitting 30 EEG classification jobs..."
echo "================================================"

"""

    for i, job_file in enumerate(job_files, 1):
        master_script += f"# Job {i}: {job_file}\n"
        master_script += f"sbatch experiments/eeg_job_scripts/{job_file}\n"
        master_script += f"echo \"Submitted: {job_file}\"\n\n"

        # Add pause every 6 jobs (one per model) to avoid overwhelming the SLURM queue
        if i % 6 == 0 and i < len(job_files):
            master_script += f"# Pause to avoid overwhelming SLURM queue\n"
            master_script += f"echo \"Pausing for 2 seconds...\"\n"
            master_script += f"sleep 2\n\n"

    master_script += """
echo "================================================"
echo "All jobs submitted!"
echo "Monitor progress with: tail -f experiments/eeg_results/logs/*.log"
echo "Or check individual logs in: experiments/eeg_results/logs/"
"""

    master_path = jobs_dir / "submit_all_eeg_jobs.sh"
    with open(master_path, 'w') as f:
        f.write(master_script)
    master_path.chmod(0o755)

    print(f"\nMaster submission script created: {master_path}")

    # Create logs directory
    logs_dir = Path(OUTPUT_DIR) / "logs"
    logs_dir.mkdir(parents=True, exist_ok=True)
    print(f"Logs directory created: {logs_dir}")

    print("\n" + "="*80)
    print("Usage Instructions:")
    print("="*80)
    print("1. Submit all jobs in parallel:")
    print(f"   bash {master_path}")
    print()
    print("2. Submit individual jobs:")
    print(f"   bash experiments/eeg_job_scripts/eeg_quantum_hydra_seed2024.sh")
    print()
    print("3. Monitor progress:")
    print(f"   tail -f experiments/eeg_results/logs/*.log")
    print()
    print("4. After completion, aggregate results:")
    print(f"   python experiments/aggregate_eeg_results.py")
    print("="*80)


if __name__ == "__main__":
    main()
